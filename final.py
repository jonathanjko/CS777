# -*- coding: utf-8 -*-
"""Final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r275SZ2w9jIdpI2iYwoV79BXme_5ZSIj
"""

import os
import json
import sys
import re
import pandas as pd
import seaborn as sns
import numpy as np
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, unix_timestamp, explode, lag, sqrt, when, radians, asin, cos, sin, count, avg
from pyspark.sql.window import Window
from pyspark.ml.feature import VectorAssembler, StringIndexer
from pyspark.ml.clustering import KMeans
from pyspark.ml.classification import RandomForestClassifier
from pyspark.ml.evaluation import ClusteringEvaluator, MulticlassClassificationEvaluator
import matplotlib.pyplot as plt

def read_json_to_df(json_path, spark):
    """Reads a JSON file into a Spark DataFrame."""
    print(f"Reading JSON file from: {json_path}")
    # load the json file into a spark dataframe
    df = spark.read.option("multiline", "true").json(json_path)
    return df

if __name__ == "__main__":
#  if len(sys.argv) != 3:
#    print("Usage: TaxiAssignment <inputfile> <output> ", file=sys.stderr)
#    exit(-1)

#     if len(sys.argv) != 3:
#         print("Usage: wordcount <file> <output> ", file=sys.stderr)
#         exit(-1)
# SparkContext(master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)
  # initialize spark session
  spark = SparkSession.builder \
    .appName("ModeOfTransportDetection") \
    .getOrCreate()

  # path to the json file
  json_file_path = sys.argv[1]
  output_dir = sys.argv[2]
  print(f"Processing JSON file: {json_file_path}")
  # read json file into dataframe
  df = read_json_to_df(json_file_path, spark)
  df.head()

  # explode the nested structure to flatten the dataframe
  df = df.withColumn("trajectory", explode(col("trajectory")))


  # select the required columns from the nested structure
  df = df.select(
      col("user_id"),
      col("trajectory.latitude").alias("latitude"),
      col("trajectory.longitude").alias("longitude"),
      col("trajectory.altitude").alias("altitude"),
      col("trajectory.timestamp").alias("timestamp")
      #col("trajectory.mode").alias("mode")
    )
  # filter out trajectories with 'unknown' mode
  #df = df.filter(df.mode != "unknown")

  print("Converting timestamps to Spark TimestampType")
  # convert timestamp to spark timestamp type
  df = df.withColumn("timestamp", unix_timestamp(col("timestamp"), "yyyy-MM-dd HH:mm:ss").cast("timestamp"))
  # cache the dataframe to avoid recomputation and improve performance
  df.cache()

  # show the schema and sample data
  print("Schema of DataFrame:")
  df.printSchema()
  print("Sample data:")
  df.show(5)

  # summary statistics
  print("Summary statistics:")
  df.describe().show()

  # calculate time differences and distances using window functions
  print("Calculating time differences and distances")
  window_spec = Window.partitionBy("user_id").orderBy("timestamp")

  df = df.withColumn("prev_latitude", lag("latitude").over(window_spec))
  df = df.withColumn("prev_longitude", lag("longitude").over(window_spec))
  df = df.withColumn("prev_timestamp", lag("timestamp").over(window_spec))

  df = df.withColumn("time_diff", (col("timestamp").cast("long") - col("prev_timestamp").cast("long")))

  # calculate haversine distance, which is more accurate for long distances on the earth's surface
  print("Calculating Haversine distances")
  df = df.withColumn("lat1_rad", radians(col("latitude")))
  df = df.withColumn("lat2_rad", radians(col("prev_latitude")))
  df = df.withColumn("delta_lat", radians(col("latitude") - col("prev_latitude")))
  df = df.withColumn("delta_long", radians(col("longitude") - col("prev_longitude")))

  df = df.withColumn("a", sin(col("delta_lat") / 2) ** 2 + cos(col("lat1_rad")) * cos(col("lat2_rad")) * sin(col("delta_long") / 2) ** 2)
  df = df.withColumn("c", 2 * asin(sqrt(col("a"))))

  earth_radius_km = 6371
  df = df.withColumn("distance", col("c") * earth_radius_km * 1000)  # Convert km to meters

  #df = df.withColumn("lat_diff", col("latitude") - col("prev_latitude"))
  #df = df.withColumn("long_diff", col("longitude") - col("prev_longitude"))

  # calculate distance (assuming spherical Earth)
  #print("Calculating distances")
  #df = df.withColumn("distance", sqrt(col("lat_diff")**2 + col("long_diff")**2))

  # Use haversine_distance for speed calculation
  print("Calculating speeds")
  df = df.withColumn("speed", when(col("time_diff") > 0, col("distance") / col("time_diff")).otherwise(0))

  # calculate speed (distance / time)
  #print("Calculating speeds")
  #df = df.withColumn("speed", when(col("time_diff") > 0, col("distance") / col("time_diff")).otherwise(0))

  # remove null values from the first calculation
  print("Removing null values")
  df = df.na.drop()

  # calculate acceleration (change in speed over time)
  print("Calculating accelerations")
  df = df.withColumn("prev_speed", lag("speed").over(window_spec))
  df = df.withColumn("acceleration", when(col("time_diff") > 0, (col("speed") - col("prev_speed")) / col("time_diff")).otherwise(0))

  # fill null values for the first row where previous calculations cannot be made
  df = df.fillna({'speed': 0, 'acceleration': 0})

  # Aggregate user-level features
  print("Aggregating user-level features")
  user_features = df.groupBy("user_id").agg(
      avg("speed").alias("avg_speed"),
      avg("acceleration").alias("avg_acceleration"),
      avg("distance").alias("avg_distance")
  )

  results = []
  # show the processed data
  print("Processed data:")
  df.select("user_id", "timestamp", "latitude", "longitude", "speed", "acceleration").show(5)
  processed_sample = df.select("user_id", "timestamp", "latitude", "longitude", "speed", "acceleration").take(5)
  results.append("Processed Sample Data:")
  for row in processed_sample:
    results.append(row)
  results.append("\n")

  # clustering with K-Means
  print("Performing clustering with K-Means")

  # prepare features for clustering
  assembler = VectorAssembler(inputCols=["avg_speed", "avg_acceleration", "avg_distance"], outputCol="features")
  user_features = assembler.transform(user_features)

  # apply K-Means clustering
  kmeans = KMeans(k=3, seed=1)
  model = kmeans.fit(user_features)

  # make predictions
  predictions = model.transform(user_features)

  # evaluate clustering
  evaluator = ClusteringEvaluator()
  silhouette = evaluator.evaluate(predictions)
  print(f"Silhouette with squared euclidean distance: {silhouette}")
  results.append(f"Silhouette with squared euclidean distance: {silhouette}")
  results.append("\n")

  # visualize clustering results
  print("Visualizing clustering results")
  sample_predictions = predictions.sample(fraction=0.1).collect()

  plt.figure(figsize=(12, 6))
  for cluster_id in range(3):
    cluster_points = [(row['avg_speed'], row['avg_acceleration']) for row in sample_predictions if row['prediction'] == cluster_id]
    if cluster_points:  # check if there are any points in the cluster
      speeds, accelerations = zip(*cluster_points)
      plt.scatter(speeds, accelerations, label=f'Cluster {cluster_id}', alpha=0.5)

  plt.title('K-Means Clustering of Trajectories')
  plt.xlabel('Average Speed')
  plt.ylabel('Average Acceleration')
  plt.legend()
  plt.display()
  plt.savefig("kmeansclustering.png")  # Save plot
  plt.close()

  # classification with Random Forest using the mode label
  print("Performing classification with Random Forest")
  df = df.withColumn("mode",
                     when(df.speed < 5, "walking")
                     .when((df.speed >= 5) & (df.speed < 25), "cycling")
                     .when(df.speed >= 25, "driving")
                     .otherwise("stationary")
  )
  # encode the mode column to use it as a label for classification
  print("Encoding mode column")
  indexer = StringIndexer(inputCol="mode", outputCol="label")
  df_indexed = indexer.fit(df).transform(df)

  # prepare features for classification
  assembler_classification = VectorAssembler(inputCols=["speed", "acceleration", "distance"], outputCol="features")
  df_features = assembler_classification.transform(df_indexed)

  # split data into training and test sets
  train_df, test_df = df_features.randomSplit([0.8, 0.2], seed=3192)
  # save train and test DataFrames as CSV
  #train_df.write.csv(output_dir, header=True, mode='overwrite')
  #test_df.write.csv(output_dir, header=True, mode='overwrite')

  # train Random Forest model
  rf = RandomForestClassifier(labelCol="label", featuresCol="features", numTrees=10)
  rf_model = rf.fit(train_df)

  # make predictions
  predictions = rf_model.transform(test_df)
  # here are the predictions
  print("Predictions")
  #predictions.show(5)

  # evaluate classification
  evaluator = MulticlassClassificationEvaluator(labelCol="label", predictionCol="prediction", metricName="accuracy")
  accuracy = evaluator.evaluate(predictions)
  print(f"Test Accuracy = {accuracy}")
  results.append(f"Test Accuracy = {accuracy}")
  results.append("\n")

  # calculate and print the confusion matrix
  print("calculating confusion matrix")
  confusion_matrix = predictions.groupBy("label", "prediction").agg(count("*").alias("count"))
  # convert the confusion matrix to a format suitable for plotting
  confusion_matrix_pandas = confusion_matrix.toPandas()
  confusion_matrix_pivot = confusion_matrix_pandas.pivot(index='label', columns='prediction', values='count').fillna(0).astype(int)

  # append to results list
  results.append("Confusion Matrix:\n")
  results.append(confusion_matrix_pivot.to_string())
  plt.figure(figsize=(8, 6))
  sns.heatmap(confusion_matrix_pivot, annot=True, fmt='d', cmap='Blues')
  plt.xlabel('Predicted')
  plt.ylabel('Actual')
  plt.title('Confusion Matrix')
  plt.display()
  plt.savefig("confusionmatrix.png")
  plt.show()

  # calculate TN, TP, FN, FP for each class
  class_names = indexer.fit(df).labels

  # initialize arrays to store metrics
  tp = np.diag(confusion_matrix_pivot)  # True Positives are the diagonal elements
  fp = confusion_matrix_pivot.sum(axis=0) - tp  # False Positives
  fn = confusion_matrix_pivot.sum(axis=1) - tp  # False Negatives
  tn = confusion_matrix_pivot.values.sum() - (tp + fp + fn)  # True Negatives
  # show results
  for i, class_name in enumerate(class_names):
    print(f"Class '{class_name}':")
    print(f"  TP: {tp[i]}")
    print(f"  FP: {fp[i]}")
    print(f"  FN: {fn[i]}")
    print(f"  TN: {tn[i]}")
    print()
  # append TN, TP, FN, FP metrics to the results list
  for i, class_name in enumerate(class_names):
    results.append(f"Class '{class_name}':\n")
    results.append(f"  TP: {tp[i]}\n")
    results.append(f"  FP: {fp[i]}\n")
    results.append(f"  FN: {fn[i]}\n")
    results.append(f"  TN: {tn[i]}\n")
    results.append("\n")

  # plot speed vs. distance
  print("Plotting speed vs. distance")
  plt.figure(figsize=(12, 6))
  speed_distance_data = df.select("speed", "distance").sample(fraction=0.1).collect()
  plt.scatter([row['speed'] for row in speed_distance_data], [row['distance'] for row in speed_distance_data], alpha=0.5)
  plt.title('Speed vs. Distance')
  plt.xlabel('Speed')
  plt.ylabel('Distance')
  plt.display()
  plt.savefig("speedvdistance.png")  # Save plot
  plt.show()

  # visualize trajectory data distribution for all users
  print("Visualizing trajectory points for all users")
  all_users = df.select("user_id").distinct().collect()

  for user in all_users:
    user_id = user['user_id']
    sample_data = df.filter(df.user_id == user_id).select("latitude", "longitude").sample(fraction=0.1).collect()

    plt.figure(figsize=(10, 6))
    plt.scatter([row['longitude'] for row in sample_data], [row['latitude'] for row in sample_data], alpha=0.5)
    plt.title(f'Trajectory Points Distribution for User {user_id}')
    plt.xlabel('Longitude')
    plt.ylabel('Latitude')
    # unique file name for each user's plot
    plt.display()
    plot_filename = f'trajectory_user_{user_id}.png'
    plt.savefig(plot_filename)  # Save plot
    plt.show()

  # calculate summary statistics using Spark
  print("Calculating summary statistics using Spark")
  df.describe(['latitude', 'longitude', 'altitude', 'speed', 'acceleration', 'timestamp']).show()

  # Create an RDD from the results list
  results_rdd = spark.sparkContext.parallelize(results)
  # Coalesce to a single partition
  results_rdd = results_rdd.coalesce(1)
  # Save the RDD to a text file
  results_rdd.saveAsTextFile(output_dir)

  spark.stop()